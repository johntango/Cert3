{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    },
    "colab": {
      "name": "2020-07-16-02-Building-a-fake-news-classifier.ipynb",
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/johntango/Cert3/blob/main/_notebooks/2020-07-16-02-Building-a-fake-news-classifier.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xkIC5fFR8l51"
      },
      "source": [
        "# Building a fake news classifier\n",
        "> You'll apply the basics of what you've learned along with some supervised machine learning to build a \"fake news\" detector. You'll begin by learning the basics of supervised machine learning, and then move forward by choosing a few important features and testing ideas to identify and classify fake news articles. This is the Summary of lecture \"Introduction to Natural Language Processing in Python\", via datacamp.\n",
        "\n",
        "- toc: true \n",
        "- badges: true\n",
        "- comments: true\n",
        "- author: Chanseok Kang\n",
        "- categories: [Python, Datacamp, Natural_Language_Processing]\n",
        "- image: "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qO9dh_yQ-Jeg"
      },
      "source": [
        "Example of CountVector dt idf\n",
        "**bold text**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ejPKQgKg-P_5",
        "outputId": "19648619-111f-4bcb-ffde-f8365744e25f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "  \n",
        "document = [\"One Geek helps Two Geeks\",\n",
        "            \"Two Geeks help Four Geeks\",\n",
        "            \"Each Geek helps many other Geeks at GeeksforGeeks\"]\n",
        "  \n",
        "# Create a Vectorizer Object\n",
        "vectorizer = CountVectorizer()\n",
        "  \n",
        "vectorizer.fit(document)\n",
        "  \n",
        "# Printing the identified Unique words along with their indices\n",
        "print(\"Vocabulary: \", vectorizer.vocabulary_)\n",
        "  \n",
        "# Encode the Document\n",
        "vector = vectorizer.transform(document)\n",
        "  \n",
        "# Summarizing the Encoded Texts\n",
        "print(\"Encoded Document is:\")\n",
        "print(vector.toarray())"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Vocabulary:  {'one': 9, 'geek': 3, 'helps': 7, 'two': 11, 'geeks': 4, 'help': 6, 'four': 2, 'each': 1, 'many': 8, 'other': 10, 'at': 0, 'geeksforgeeks': 5}\n",
            "Encoded Document is:\n",
            "[[0 0 0 1 1 0 0 1 0 1 0 1]\n",
            " [0 0 1 0 2 0 1 0 0 0 0 1]\n",
            " [1 1 0 1 1 1 0 1 1 0 1 0]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "coefSRYz8l53"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1GV3AVuc8l54"
      },
      "source": [
        "## Classifying fake news using supervised learning with NLP\n",
        "- Supervised learning with NLP\n",
        "    - Need to use language instead of geometric features\n",
        "    - Use bag-of-words models or tf-idf features\n",
        "- Supervised learning steps\n",
        "    - Collect and preprocess our data\n",
        "    - Determine a label\n",
        "    - Split data into training and test sets\n",
        "    - Extract features from the text to help predict the label\n",
        "    - Evaluate trained model using test set"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bMjlNole8l54"
      },
      "source": [
        "## Building word count vectors with scikit-learn\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Am8pmqvL8l54"
      },
      "source": [
        "### CountVectorizer for text classification\n",
        "It's time to begin building your text classifier! The [data](https://s3.amazonaws.com/assets.datacamp.com/production/course_3629/fake_or_real_news.csv) has been loaded into a DataFrame called df. The `.head()` method is particularly informative.\n",
        "\n",
        "In this exercise, you'll use `pandas` alongside scikit-learn to create a sparse text vectorizer you can use to train and test a simple supervised model. To begin, you'll set up a `CountVectorizer` and investigate some of its features."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bHgmwTwN8l54",
        "outputId": "09c98351-5c2d-43bc-8414-d05aebcef9fb"
      },
      "source": [
        "df = pd.read_csv('./dataset/fake_or_real_news.csv')\n",
        "df.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Unnamed: 0</th>\n",
              "      <th>title</th>\n",
              "      <th>text</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>8476</td>\n",
              "      <td>You Can Smell Hillary’s Fear</td>\n",
              "      <td>Daniel Greenfield, a Shillman Journalism Fello...</td>\n",
              "      <td>FAKE</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>10294</td>\n",
              "      <td>Watch The Exact Moment Paul Ryan Committed Pol...</td>\n",
              "      <td>Google Pinterest Digg Linkedin Reddit Stumbleu...</td>\n",
              "      <td>FAKE</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3608</td>\n",
              "      <td>Kerry to go to Paris in gesture of sympathy</td>\n",
              "      <td>U.S. Secretary of State John F. Kerry said Mon...</td>\n",
              "      <td>REAL</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>10142</td>\n",
              "      <td>Bernie supporters on Twitter erupt in anger ag...</td>\n",
              "      <td>— Kaydee King (@KaydeeKing) November 9, 2016 T...</td>\n",
              "      <td>FAKE</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>875</td>\n",
              "      <td>The Battle of New York: Why This Primary Matters</td>\n",
              "      <td>It's primary day in New York and front-runners...</td>\n",
              "      <td>REAL</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   Unnamed: 0                                              title  \\\n",
              "0        8476                       You Can Smell Hillary’s Fear   \n",
              "1       10294  Watch The Exact Moment Paul Ryan Committed Pol...   \n",
              "2        3608        Kerry to go to Paris in gesture of sympathy   \n",
              "3       10142  Bernie supporters on Twitter erupt in anger ag...   \n",
              "4         875   The Battle of New York: Why This Primary Matters   \n",
              "\n",
              "                                                text label  \n",
              "0  Daniel Greenfield, a Shillman Journalism Fello...  FAKE  \n",
              "1  Google Pinterest Digg Linkedin Reddit Stumbleu...  FAKE  \n",
              "2  U.S. Secretary of State John F. Kerry said Mon...  REAL  \n",
              "3  — Kaydee King (@KaydeeKing) November 9, 2016 T...  FAKE  \n",
              "4  It's primary day in New York and front-runners...  REAL  "
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XphrAM5J8l55",
        "outputId": "51deb997-9b32-487c-93f3-4b6792e80d8b"
      },
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Create a series to store the labels: y\n",
        "y = df.label\n",
        "\n",
        "# Create training set and test set\n",
        "X_train, X_test, y_train, y_test = train_test_split(df['text'], y, \n",
        "                                                    test_size=0.33, random_state=53)\n",
        "\n",
        "# Initialize a CountVectorizer object: count_vectorizer\n",
        "count_vectorizer = CountVectorizer(stop_words='english')\n",
        "\n",
        "# Transform the training data using only the 'text' column values: count_train\n",
        "count_train = count_vectorizer.fit_transform(X_train)\n",
        "\n",
        "# Transform the test data using only the 'text' column values: count_test\n",
        "count_test = count_vectorizer.transform(X_test)\n",
        "\n",
        "# Print the first 10 features of the count_vectorizer\n",
        "print(count_vectorizer.get_feature_names()[:10])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['00', '000', '0000', '00000031', '000035', '00006', '0001', '0001pt', '000ft', '000km']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7g5HHj6-8l56"
      },
      "source": [
        "### TfidfVectorizer for text classification\n",
        "Similar to the sparse `CountVectorizer` created in the previous exercise, you'll work on creating tf-idf vectors for your documents. You'll set up a `TfidfVectorizer` and investigate some of its features."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KenackVSB-29"
      },
      "source": [
        "# TfIdf \n",
        "If a word occurs in all documents then it contains little classification power. Inverse document frequency weights the \"term frequency\" to account for this. \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vAzB70XG8l56",
        "outputId": "6a3c8950-5420-4bc8-841a-d81f1d22c326"
      },
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "# Initialize a TfidfVectorizer object: tfidf_vectorizer\n",
        "tfidf_vectorizer = TfidfVectorizer(stop_words='english', max_df=0.7)\n",
        "\n",
        "# Transform the training data: tfidf_train\n",
        "tfidf_train = tfidf_vectorizer.fit_transform(X_train)\n",
        "\n",
        "# transform the test data: tfidf_test\n",
        "tfidf_test = tfidf_vectorizer.transform(X_test)\n",
        "\n",
        "# Print the first 10 features\n",
        "print(tfidf_vectorizer.get_feature_names()[:10])\n",
        "\n",
        "# Print the first 5 vectors of the tfidf training data\n",
        "print(tfidf_train.A[:5])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['00', '000', '0000', '00000031', '000035', '00006', '0001', '0001pt', '000ft', '000km']\n",
            "[[0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lMafxMcK8l56"
      },
      "source": [
        "### Inspecting the vectors\n",
        "To get a better idea of how the vectors work, you'll investigate them by converting them into pandas DataFrames."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EQl3fMro8l57",
        "outputId": "87d81e61-6fe1-46f3-beaf-190cab322950"
      },
      "source": [
        "# Create the CountVectorizer DataFrame: count_df\n",
        "count_df = pd.DataFrame(count_train.A, columns=count_vectorizer.get_feature_names())\n",
        "\n",
        "# Create the TfidfVectorizer DataFrame: tfidf_df\n",
        "tfidf_df = pd.DataFrame(tfidf_train.A, columns=tfidf_vectorizer.get_feature_names())\n",
        "\n",
        "# Print the head of count_df\n",
        "print(count_df.head())\n",
        "\n",
        "# Print the head of tfidf_df\n",
        "print(tfidf_df.head())\n",
        "\n",
        "# Calculate the difference in columns: difference\n",
        "difference = set(count_df.columns) - set(tfidf_df.columns)\n",
        "print(difference)\n",
        "\n",
        "# Check whether the DataFrame are equal\n",
        "print(count_df.equals(tfidf_df))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "   00  000  0000  00000031  000035  00006  0001  0001pt  000ft  000km  ...  \\\n",
            "0   0    0     0         0       0      0     0       0      0      0  ...   \n",
            "1   0    0     0         0       0      0     0       0      0      0  ...   \n",
            "2   0    0     0         0       0      0     0       0      0      0  ...   \n",
            "3   0    0     0         0       0      0     0       0      0      0  ...   \n",
            "4   0    0     0         0       0      0     0       0      0      0  ...   \n",
            "\n",
            "   حلب  عربي  عن  لم  ما  محاولات  من  هذا  والمرضى  ยงade  \n",
            "0    0     0   0   0   0        0   0    0        0      0  \n",
            "1    0     0   0   0   0        0   0    0        0      0  \n",
            "2    0     0   0   0   0        0   0    0        0      0  \n",
            "3    0     0   0   0   0        0   0    0        0      0  \n",
            "4    0     0   0   0   0        0   0    0        0      0  \n",
            "\n",
            "[5 rows x 56922 columns]\n",
            "    00  000  0000  00000031  000035  00006  0001  0001pt  000ft  000km  ...  \\\n",
            "0  0.0  0.0   0.0       0.0     0.0    0.0   0.0     0.0    0.0    0.0  ...   \n",
            "1  0.0  0.0   0.0       0.0     0.0    0.0   0.0     0.0    0.0    0.0  ...   \n",
            "2  0.0  0.0   0.0       0.0     0.0    0.0   0.0     0.0    0.0    0.0  ...   \n",
            "3  0.0  0.0   0.0       0.0     0.0    0.0   0.0     0.0    0.0    0.0  ...   \n",
            "4  0.0  0.0   0.0       0.0     0.0    0.0   0.0     0.0    0.0    0.0  ...   \n",
            "\n",
            "   حلب  عربي   عن   لم   ما  محاولات   من  هذا  والمرضى  ยงade  \n",
            "0  0.0   0.0  0.0  0.0  0.0      0.0  0.0  0.0      0.0    0.0  \n",
            "1  0.0   0.0  0.0  0.0  0.0      0.0  0.0  0.0      0.0    0.0  \n",
            "2  0.0   0.0  0.0  0.0  0.0      0.0  0.0  0.0      0.0    0.0  \n",
            "3  0.0   0.0  0.0  0.0  0.0      0.0  0.0  0.0      0.0    0.0  \n",
            "4  0.0   0.0  0.0  0.0  0.0      0.0  0.0  0.0      0.0    0.0  \n",
            "\n",
            "[5 rows x 56922 columns]\n",
            "set()\n",
            "False\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pzeLyOe58l57"
      },
      "source": [
        "## Training and testing a classification model with scikit-learn"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I3LcvtTl8l57"
      },
      "source": [
        "### Training and testing the \"fake news\" model with CountVectorizer\n",
        "Now it's your turn to train the \"fake news\" model using the features you identified and extracted. In this first exercise you'll train and test a Naive Bayes model using the CountVectorizer data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HH88Cf7P8l57",
        "outputId": "ee6ba262-33cd-4e73-fd3c-5a9f7e7ba5ae"
      },
      "source": [
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix\n",
        "\n",
        "# Instantiate a Multinomial Naive Bayes classifier: nb_classifier\n",
        "nb_classifier = MultinomialNB()\n",
        "\n",
        "# Fit the classifier to the training data\n",
        "nb_classifier.fit(count_train, y_train)\n",
        "\n",
        "# Create the predicted tags: pred\n",
        "pred = nb_classifier.predict(count_test)\n",
        "\n",
        "# Calculate the accuracy score: score\n",
        "score = accuracy_score(y_test, pred)\n",
        "print(score)\n",
        "\n",
        "# Calculate the confusion matrix: cm\n",
        "cm =confusion_matrix(y_test, pred, labels=['FAKE', 'REAL'])\n",
        "print(cm)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.893352462936394\n",
            "[[ 865  143]\n",
            " [  80 1003]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "orstQSB58l58"
      },
      "source": [
        "### Training and testing the \"fake news\" model with TfidfVectorizer\n",
        "Now that you have evaluated the model using the `CountVectorizer`, you'll do the same using the `TfidfVectorizer` with a Naive Bayes model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jhm7dl9l8l58",
        "outputId": "90ee1f85-93e2-47e0-900c-3d52c8454ceb"
      },
      "source": [
        "# Create a Multinomial Naive Bayes classifier: nb_classifier\n",
        "nb_classifier = MultinomialNB()\n",
        "\n",
        "# Fit the classifier to the training data\n",
        "nb_classifier.fit(tfidf_train, y_train)\n",
        "\n",
        "# Create the predicted tags: pred\n",
        "pred = nb_classifier.predict(tfidf_test)\n",
        "\n",
        "# Calculate the accuracy score: score\n",
        "score = accuracy_score(y_test, pred)\n",
        "print(score)\n",
        "\n",
        "# Calculate the confusion matrix: cm\n",
        "cm = confusion_matrix(y_test, pred, labels=['FAKE', 'REAL'])\n",
        "print(cm)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.8565279770444764\n",
            "[[ 739  269]\n",
            " [  31 1052]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B4E-BzlE8l59"
      },
      "source": [
        "## Simple NLP, complex problems\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FwkpYUNQ8l59"
      },
      "source": [
        "### Improving your model\n",
        "Your job in this exercise is to test a few different alpha levels using the Tfidf vectors to determine if there is a better performing combination."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B4G5ix008l59",
        "outputId": "3ef78535-d84a-490b-c4d6-afcfb08df31c"
      },
      "source": [
        "# Create the list of alphas: alphas\n",
        "alphas = np.arange(0, 1, 0.1)\n",
        "\n",
        "# Define train_and_predict()\n",
        "def train_and_predict(alpha):\n",
        "    # Instantiate the classifier: nb_classifier\n",
        "    nb_classifier = MultinomialNB(alpha=alpha)\n",
        "    \n",
        "    # Fit to the training data\n",
        "    nb_classifier.fit(tfidf_train, y_train)\n",
        "    \n",
        "    # Predict the labels: pred\n",
        "    pred = nb_classifier.predict(tfidf_test)\n",
        "    \n",
        "    # Compute accuracy: score\n",
        "    score = accuracy_score(y_test, pred)\n",
        "    return score\n",
        "\n",
        "# Iterate over the alphas and print the corresponding score\n",
        "for alpha in alphas:\n",
        "    print('Alpha: ', alpha)\n",
        "    print('Score: ', train_and_predict(alpha))\n",
        "    print()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Alpha:  0.0\n",
            "Score:  0.8813964610234337\n",
            "\n",
            "Alpha:  0.1\n",
            "Score:  0.8976566236250598\n",
            "\n",
            "Alpha:  0.2\n",
            "Score:  0.8938307030129125\n",
            "\n",
            "Alpha:  0.30000000000000004\n",
            "Score:  0.8900047824007652\n",
            "\n",
            "Alpha:  0.4\n",
            "Score:  0.8857006217120995\n",
            "\n",
            "Alpha:  0.5\n",
            "Score:  0.8842659014825442\n",
            "\n",
            "Alpha:  0.6000000000000001\n",
            "Score:  0.874701099952176\n",
            "\n",
            "Alpha:  0.7000000000000001\n",
            "Score:  0.8703969392635102\n",
            "\n",
            "Alpha:  0.8\n",
            "Score:  0.8660927785748446\n",
            "\n",
            "Alpha:  0.9\n",
            "Score:  0.8589191774270684\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/home/chanseok/anaconda3/lib/python3.7/site-packages/sklearn/naive_bayes.py:507: UserWarning: alpha too small will result in numeric errors, setting alpha = 1.0e-10\n",
            "  'setting alpha = %.1e' % _ALPHA_MIN)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z_-cWQI38l5-"
      },
      "source": [
        "### Inspecting your model\n",
        "Now that you have built a \"fake news\" classifier, you'll investigate what it has learned. You can map the important vector weights back to actual words using some simple inspection techniques."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6uWDlFrq8l5-",
        "outputId": "84c44e27-3873-4729-bcba-17143b80c3d7"
      },
      "source": [
        "# Get the class labels: class_labels\n",
        "class_labels = nb_classifier.classes_\n",
        "\n",
        "# Extract the features: feature_names\n",
        "feature_names = tfidf_vectorizer.get_feature_names()\n",
        "\n",
        "# Zip the feature names together with the coefficient array \n",
        "# and sort by weights: feat_with_weights\n",
        "feat_with_weights = sorted(zip(nb_classifier.coef_[0], feature_names))\n",
        "\n",
        "# Print the first class label and the top 20 feat_with_weights entries\n",
        "print(class_labels[0], feat_with_weights[:20])\n",
        "\n",
        "# Print the second class label and the bottom 20 feat_with_weights entries\n",
        "print(class_labels[1], feat_with_weights[-20:])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "FAKE [(-11.316312804238807, '0000'), (-11.316312804238807, '000035'), (-11.316312804238807, '0001'), (-11.316312804238807, '0001pt'), (-11.316312804238807, '000km'), (-11.316312804238807, '0011'), (-11.316312804238807, '006s'), (-11.316312804238807, '007'), (-11.316312804238807, '007s'), (-11.316312804238807, '008s'), (-11.316312804238807, '0099'), (-11.316312804238807, '00am'), (-11.316312804238807, '00p'), (-11.316312804238807, '00pm'), (-11.316312804238807, '014'), (-11.316312804238807, '015'), (-11.316312804238807, '018'), (-11.316312804238807, '01am'), (-11.316312804238807, '020'), (-11.316312804238807, '023')]\n",
            "REAL [(-7.742481952533027, 'states'), (-7.717550034444668, 'rubio'), (-7.703583809227384, 'voters'), (-7.654774992495461, 'house'), (-7.649398936153309, 'republicans'), (-7.6246184189367, 'bush'), (-7.616556675728881, 'percent'), (-7.545789237823644, 'people'), (-7.516447881078008, 'new'), (-7.448027933291952, 'party'), (-7.411148410203476, 'cruz'), (-7.410910239085596, 'state'), (-7.35748985914622, 'republican'), (-7.33649923948987, 'campaign'), (-7.2854057032685775, 'president'), (-7.2166878130917755, 'sanders'), (-7.108263114902301, 'obama'), (-6.724771332488041, 'clinton'), (-6.5653954389926845, 'said'), (-6.328486029596207, 'trump')]\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}